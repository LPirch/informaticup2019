%!TEX root = paper.tex"`

\section{Evaluation}

To further evaluate the quality of the attacks, we generated multiple adversarial examples with different configurations and classified them using the \textbf{remote} model. 
In the following, we briefly discuss our setup and the selection of the attack parameters. Afterwards, we present and discuss the attack results.

\subsection{Setup}

\begin{table}
\begin{tabular}{l | l}
\textbf{Parameter} & \textbf{Values} \\\hline

target & 0,2,3,5,11,12,16,27,31,33 \\\hline

model & gtsrb\_model, jbda3\\\hline

attack & cwl2, robust\_cwl2\\\hline

image & abbey, gi, octocat\\\hline

conf & 15, 20
\end{tabular}
\caption{Attack Configuration}
\label{tab:cli_params}
\end{table}

\begin{minipage}{\linewidth}
An adversarial example is generated using the command line call:

\vspace{2ex}
\texttt{python3 attack\_model.py ----target [target]}

\hspace{4cm}\texttt{----model [model]}

\hspace{4cm}\texttt{----attack [attack]}

\hspace{4cm}\texttt{----image [image].png}

\hspace{4cm}\texttt{----confidence [conf]}

\hspace{4cm}\texttt{----max\_iterations 7500}

\hspace{4cm}\texttt{----binary\_search\_steps 12}
\vspace{2ex}
\end{minipage}

Arguments which are embraced by parentheses are taken from Table \ref{tab:cli_params}. 
If the Carlini \& Wagner attack (cwl2) is chosen as the attack technique, this attempts to create one adversarial example. 
Otherwise, if our modified Carlini \& Wagner attack (robust\_cwl2) is used, five adversarial examples are generated.\footnote{Please note, that the 'modified Carlini \& Wagner' attack from Section \ref{subsec:cwl2_mod} is internally referred to as robust\_cwl2} We omitted the Robust Physical Perturbation attack (RP$_2$) from our evaluation, because the success of the attack heavily depends on the masking image which might skew the results.

The attack\_model script only returns \emph{successful} adversarial examples, otherwise generating no image. 
For the Carlini \& Wagner attack, an adversarial example is successful if the local model classifies it as the target class. 
The modified Carlini \& Wagner attack only returns adversarial examples if the center image (where no perspective transformation is applied) is classified as the target class.

For our evaluation, we chose ten random labels as our target classes to keep the computation at a moderate level while still receiving meaningful results. 
The models under attack were created using the techniques from Section \ref{subsec:modelstealing}: \emph{Rebuilding} was used for the gtsrb model and \emph{Stealing} with Jacobian based dataset augmentation was used for the jbda model. 

The results of the attacks are displayed in Table \ref{tab:cwl2_result} and Table \ref{tab:robust_result}. 
Each cell shows the number of adversarial examples in the respective category. 
To illustrate this for the modified Carlini \& Wagner attack results (Table \ref{tab:robust_result}): 37 adversarial examples of the \enquote{abbey} image (which were generated with a confidence parameter of 20 against the gtsrb model) were classified with a confidence higher than 90\% by the remote model for the target class. 
On the other hand, four adversarial examples did not transfer and received a target-score lower than 90\%. 
Recall, that the modified C\&W attack is able to produce five adversarial example in one run. 
Therefore, the attack is potentially able to create 50 adversarial examples for the ten given targets. 
Thus, 43 of the potential 50 adversarial examples were successfully created on the local model.

In the following, we use the notation:

\begin{itemize}
\item[-] $P(C)$ = Probability of an adversarial example being successfully \textbf{created} on a local model
\item[-] $P(T)$ = Probability of an adversarial example \textbf{transferring} to the remote model (i.e. classification $\geq90\%$)
\item[-] $P(G)$ = Probability that the adversarial example was generated on the local \textbf{gtsrb model}
\item[-] $P(J)$ = Probability that the adversarial example was generated on the local \textbf{jbda model}
\end{itemize}

\subsection{Carlini \& Wagner}


\begin{table}
\begin{tabular}{l l | r r | r r | r r}
& & abbey & & gi & & octo & \\[1ex]
& & \footnotesize$\geq90\%$ & \footnotesize$<90\%$ & \footnotesize$\geq90\%$ & \footnotesize$<90\%$ & \footnotesize$\geq90\%$ & \footnotesize$<90\%$ \\[1ex]
\hline
gtsrb & 15 & 8 & 2 & 7 & 2 & 7 & 0 \\[1ex]
& 20 & 10 & 0 & 8 & 1 & 7 & 2 \\[1ex]
\hline
jbda & 15 & 6 & 3 & 1 & 3 & 6 & 1 \\[1ex]
& 20 & 7 & 0 & 2 & 3 & 6 & 1
\end{tabular}
\caption{Results for the Carlini \& Wagner attack}
\label{tab:cwl2_result}
\end{table}

The Carlini \& Wagner attack is fairly successful with a general probability of creating an adversarial example for the local model of

\begin{align*}
P(C) = \frac{93}{120} = 0.775
\end{align*}

Creating an adversarial image on the gtsrb model is far more likely than on the jbda model.

\begin{align*}
P(C \mid G) = \frac{54}{60} &= 0.9\\[1ex]
P(C \mid J) = \frac{39}{60} &= 0.65
\end{align*}

Additionally, it is more probable that the adversarial example transfers from the local model to the remote model, if it has been created on the gtsrb model:

\begin{align*}
P(T \mid G \wedge C) = \frac{47}{54} &\approx 0.87\\[1ex]
P(T \mid J \wedge C) = \frac{28}{39} &\approx 0.72
\end{align*}

Overall, it is far more likely to create a transferable image on the gtsrb model than on the jbda model. 

\begin{align*}
P(T \mid G) = \frac{47}{60} &\approx 0.78 \\[1ex]
P(T \mid J) = \frac{28}{60} &\approx 0.47
\end{align*}

\subsection{Modified Carlini \& Wagner}

\begin{table}
\begin{tabular}{l l | r r | r r | r r}
& & abbey & & gi & & octo & \\[1ex]
& & \footnotesize$\geq90\%$ & \footnotesize$<90\%$ & \footnotesize$\geq90\%$ & \footnotesize$<90\%$ & \footnotesize$\geq90\%$ & \footnotesize$<90\%$ \\[1ex]
\hline
gtsrb & 15 & 36 & 7 & 30 & 2 & 34 & 5
\\[1ex]
 & 20 & 37 & 4 & 30 & 4 & 38 & 5\\[1ex]
\hline
jbda & 15 & 30 & 7 & 12 & 3 & 26 & 8
\\[1ex]
  & 20 & 23 & 1 & 14 & 1 & 23 & 1
\end{tabular}
\caption{Results for the \textit{modified} Carlini \& Wagner attack}
\label{tab:robust_result}
\end{table}

The modified Carlini \& Wagner perturbs five transformations of the source image at the same time, which places additional constraints on the generation. This is visible in the overall reduced creation success of adversarial examples on the local model in comparison to the C\&W attack:

\begin{align*}
P(C) = \frac{381}{600} = 0.635
\end{align*}

Although the attack is still more likely to create adversarial examples on the gtsrb model than on the jbda model, its probability is noticeably reduced:

\begin{align*}
P(C \mid G) = \frac{232}{300} &\approx 0.77\\[1ex]
P(C \mid J) = \frac{149}{300} &\approx 0.50
\end{align*}

However, the transferability of adversarial examples, which have been successfully created on a local model, increased for the jbda model and are unchanged for the gtsrb model. Thus, images created on either model have approximately the same probability of transferring to the remote model:

\begin{align*}
P(T \mid G \wedge C) = \frac{205}{232} &\approx 0.88\\[1ex]
P(T \mid J \wedge C) = \frac{128}{149} &\approx 0.86
\end{align*}

Overall, the gtsrb model is preferable over the jbda model for creating transferable adversarial examples in both attacks. 

\begin{align*}
P(T \mid G) = \frac{205}{300} &\approx 0.68\\[1ex]
P(T \mid J) = \frac{128}{300} &\approx 0.43
\end{align*}

To us this is quite surprising, because the jbda model used the Jacobian-based dataset augmentation for training, which is a far more sophisticated technique than the simple dataset training of the gtsrb model. 
Although the Jacobian-based dataset augmentation did not work perfectly well (it achieved a top validation accuracy of about 86\% on the remotely fetched labels), we would have expected it to better approximate the remote model.
That a more simple approach however performs better is especially remarkable as multiple classes are missing from the remote model's predictions -- as we discussed in Section \ref{subsec:modelstealing} -- and we had assumed, that this would hinder a favorable rebuilding of the remote model's decision boundary.